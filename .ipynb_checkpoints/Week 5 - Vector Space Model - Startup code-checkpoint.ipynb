{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Vector Space Model (VSM) and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the next weeks, we are going to re-implement Sherin's algorithm and apply it to the text data we've been working on last week! Here's our roadmap:\n",
    "\n",
    "**Week 5 - data cleaning**\n",
    "1. import the data\n",
    "2. clean the data (e.g., remopve stop words, punctuation, etc.)\n",
    "3. build a vocabulary for the dataset\n",
    "4. create chunks of 100 words, with a 25-words overlap\n",
    "5. create a word count matrix, where each chunk of a row and each column represents a word\n",
    "\n",
    "**Week 6 - vectorization and linear algebra**\n",
    "6. Dampen: weight the frequency of words (1 + log[count])\n",
    "7. Scale: Normalize weighted frequency of words\n",
    "8. Direction: compute deviation vectors\n",
    "\n",
    "**Week 7 - Clustering**\n",
    "9. apply different unsupervised machine learning algorithms\n",
    "    * figure out how many clusters we want to keep\n",
    "    * inspect the results of the clustering algorithm\n",
    "\n",
    "**Week 8 - Visualizing the results**\n",
    "10. create visualizations to compare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Papers/paper12.txt', 'Papers/paper5.txt', 'Papers/paper4.txt', 'Papers/paper13.txt', 'Papers/paper11.txt', 'Papers/paper6.txt', 'Papers/paper7.txt', 'Papers/paper10.txt', 'Papers/paper14.txt', 'Papers/paper3.txt', 'Papers/paper2.txt', 'Papers/paper15.txt', 'Papers/paper0.txt', 'Papers/paper1.txt', 'Papers/paper16.txt', 'Papers/paper9.txt', 'Papers/paper8.txt']\n"
     ]
    }
   ],
   "source": [
    "# 1) using glob, find all the text files in the \"Papers\" folder\n",
    "# Hint: refer to last week's notebook\n",
    "import glob\n",
    "\n",
    "txt_files = glob.glob('Papers/*.txt')\n",
    "print(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) get all the data from the text files into the \"documents\" list\n",
    "# P.S. make sure you use the 'utf-8' encoding\n",
    "documents = []\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file) as filename: \n",
    "        contents = filename.read()\n",
    "        documents.append(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "\n",
      "\f",
      "epistemic network analysis and topic modeling for chat\n",
      "data from collaborative learning environment\n",
      "zhiqiang cai\n",
      "\n",
      "brendan eagan\n",
      "\n",
      "nia m. dowell\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 410\n",
      "memphis, tn, usa\n",
      "\n",
      "university of wisconsin-madison\n",
      "1025 west johnson street\n",
      "madison, wi, usa\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 410\n",
      "memphis, tn, usa\n",
      "\n",
      "zcai@memphis.edu\n",
      "\n",
      "eaganb@gmail.com\n",
      "\n",
      "niadowell@gmail.com\n",
      "\n",
      "james w. pennebaker\n",
      "\n",
      "david w. shaffer\n",
      "\n",
      "arthur c. graesser\n",
      "\n",
      "university of texas-austin\n",
      "116 inner campus dr stop g6000\n",
      "austin, tx, usa\n",
      "\n",
      "university of wisconsin-madison\n",
      "1025 west johnson street\n",
      "madison, wi, usa\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 403\n",
      "memphis, tn, usa\n",
      "\n",
      "pennebaker@utexas.edu\n",
      "\n",
      "dws@education.wisc.edu\n",
      "\n",
      "art.graesser@gmail.com\n",
      "\n",
      "abstract\n",
      "this study investigates a possible way to analyze chat data from\n",
      "collaborative learning environments using epistemic network\n",
      "analysis and topic modeling. a 300-topic general topic model\n",
      "built from tasa\n"
     ]
    }
   ],
   "source": [
    "# 3) print the first 1000 characters of the first document to see what it \n",
    "# looks like (we'll use this as a sanity check below)\n",
    "\n",
    "print(documents[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "this study investigates a possible way to analyze chat data from\n",
      "collaborative learning environments using epistemic network\n",
      "analysis and topic modeling. a 300-topic general topic model\n",
      "built from tasa (touchstone applied science associates) corpus was used in this study. 300 topic scores for each of the 15,670\n",
      "utterances in our chat data were computed. seven relevant topics\n",
      "were selected based on the total document scores. while the aggregated topic scores had some power in predicting students‚Äô\n",
      "learning, using epistemic network analysis enables assessing the\n",
      "data from a different angle. the results showed that the topic\n",
      "score based epistemic networks between low gain students and\n",
      "high gain students were significantly different (ùë° = 2.00). overall,\n",
      "the results suggest these two analytical approaches provide complementary information and afford new insights into the processes\n",
      "related to successful collaborative interactions.\n",
      "\n",
      "keywords\n",
      "chat; collaborative learning; topic modeling; epist\n"
     ]
    }
   ],
   "source": [
    "# 4) only select the text that's between the first occurence of the \n",
    "# the word \"abstract\" and the last occurence of the word \"reference\"\n",
    "# Optional: print the length of the string before and after, as a \n",
    "# sanity check\n",
    "# HINT: https://stackoverflow.com/questions/14496006/finding-last-occurrence-of-substring-in-string-replacing-that\n",
    "# read more about rfind: https://www.tutorialspoint.com/python/string_rfind.htm\n",
    "\n",
    "def abbreviate(documents):\n",
    "    abbreviated_texts = []\n",
    "    for document in documents:\n",
    "        term1 = document.find('abstract')\n",
    "        term2 = document.rfind('reference')\n",
    "        substring = document[term1 + len('abstract'):term2]\n",
    "        abbreviated_texts.append(substring)\n",
    "    return abbreviated_texts\n",
    "\n",
    "documents = abbreviate(documents)\n",
    "print(documents[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this study investigates a possible way to analyze chat data from collaborative learning environments using epistemic network analysis and topic modeling. a 300-topic general topic model built from tasa (touchstone applied science associates) corpus was used in this study. 300 topic scores for each of the 15,670 utterances in our chat data were computed. seven relevant topics were selected based on the total document scores. while the aggregated topic scores had some power in predicting students‚Äô learning, using epistemic network analysis enables assessing the data from a different angle. the results showed that the topic score based epistemic networks between low gain students and high gain students were significantly different (ùë° = 2.00). overall, the results suggest these two analytical approaches provide complementary information and afford new insights into the processes related to successful collaborative interactions.  keywords chat; collaborative learning; topic modeling; epist\n"
     ]
    }
   ],
   "source": [
    "# 5) replace carriage returns (i.e., \"\\n\") with a white space\n",
    "# check that the result looks okay by printing the \n",
    "# first 1000 characters of the 1st doc:\n",
    "\n",
    "def strip_new_lines(documents):\n",
    "    remove_returns = []\n",
    "    for document in documents:\n",
    "        substring = document.replace('\\n', ' ')\n",
    "        remove_returns.append(substring)\n",
    "    return remove_returns \n",
    "\n",
    "documents = strip_new_lines(documents)\n",
    "print(documents[0][:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this study investigates a possible way to analyze chat data from collaborative learning environments using epistemic network analysis and topic modeling  a 300 topic general topic model built from tasa  touchstone applied science associates  corpus was used in this study  300 topic scores for each of the 15 670 utterances in our chat data were computed  seven relevant topics were selected based on the total document scores  while the aggregated topic scores had some power in predicting students  learning  using epistemic network analysis enables assessing the data from a different angle  the results showed that the topic score based epistemic networks between low gain students and high gain students were significantly different  ùë°   2 00   overall  the results suggest these two analytical approaches provide complementary information and afford new insights into the processes related to successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epist\n"
     ]
    }
   ],
   "source": [
    "# 6) replace the punctation below by a white space\n",
    "# check that the result looks okay \n",
    "# (e.g., by print the first 1000 characters of the 1st doc)\n",
    "\n",
    "punctuation_marks = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '‚àí', '‚Äù', '‚Äú', '‚Äô']\n",
    "\n",
    "def strip_punctuation(documents):\n",
    "    clean_punctuations = []\n",
    "    for document in documents:\n",
    "        for punctuation_mark in punctuation_marks:\n",
    "            document = document.replace(punctuation_mark, \" \")\n",
    "        clean_punctuations.append(document)\n",
    "    return clean_punctuations\n",
    "    \n",
    "documents = strip_punctuation(documents)\n",
    "print(documents[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this study investigates a possible way to analyze chat data from collaborative learning environments using epistemic network analysis and topic modeling  a     topic general topic model built from tasa  touchstone applied science associates  corpus was used in this study      topic scores for each of the        utterances in our chat data were computed  seven relevant topics were selected based on the total document scores  while the aggregated topic scores had some power in predicting students  learning  using epistemic network analysis enables assessing the data from a different angle  the results showed that the topic score based epistemic networks between low gain students and high gain students were significantly different  ùë°          overall  the results suggest these two analytical approaches provide complementary information and afford new insights into the processes related to successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epist\n"
     ]
    }
   ],
   "source": [
    "# 7) remove numbers by either a white space or the word \"number\"\n",
    "# again, print the first 1000 characters of the first document\n",
    "# to check that you're doing the right thing\n",
    "\n",
    "def strip_numbers(documents):\n",
    "    clean_numbers = []\n",
    "    for document in documents:\n",
    "        for char in document:\n",
    "            if char.isdigit():\n",
    "                document = document.replace(char, \" \")\n",
    "        clean_numbers.append(document)\n",
    "    return clean_numbers\n",
    "\n",
    "documents = strip_numbers(documents)\n",
    "print(documents[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling topic general topic model built tasa touchstone applied science associates corpus used study topic scores utterances chat data computed seven relevant topics selected based total document scores aggregated topic scores power predicting students learning using epistemic network analysis enables assessing data different angle results showed topic score based epistemic networks low gain students high gain students significantly different ùë° overall results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions keywords chat collaborative learning topic modeling epistemic network analysis introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synchronously asynchronously participate tasks acc\n"
     ]
    }
   ],
   "source": [
    "# 8) Remove the stop words below from our documents\n",
    "# print the first 1000 characters of the first document\n",
    "\n",
    "def strip_stopwords(documents):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', \n",
    "              'just', 'don', 'should', 'now']\n",
    "    clean_stopwords = []\n",
    "    for document in documents:\n",
    "        words = document.split()\n",
    "        resultwords  = [word for word in words if word.lower() not in stop_words]\n",
    "        result = ' '.join(resultwords)\n",
    "        clean_stopwords.append(result)\n",
    "    return clean_stopwords\n",
    "\n",
    "documents = strip_stopwords(documents)\n",
    "print(documents[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling topic general topic model built tasa touchstone applied science associates corpus used study topic scores utterances chat data computed seven relevant topics selected based total document scores aggregated topic scores power predicting students learning using epistemic network analysis enables assessing data different angle results showed topic score based epistemic networks low gain students high gain students significantly different overall results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions keywords chat collaborative learning topic modeling epistemic network analysis introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synchronously asynchronously participate tasks accom\n"
     ]
    }
   ],
   "source": [
    "# 9) remove words with one and two characters (e.g., 'd', 'er', etc.)\n",
    "# print the first 1000 characters of the first document\n",
    "import re\n",
    "\n",
    "def strip_short_words(documents):\n",
    "    clean_shortwords = []\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    for document in documents:\n",
    "        document = shortword.sub('', document)\n",
    "        clean_shortwords.append(document)\n",
    "    return clean_shortwords\n",
    "\n",
    "documents = strip_short_words(documents)\n",
    "print(documents[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) package all of your work above into a function that cleans a given document\n",
    "\n",
    "def clean_list_of_documents(documents):\n",
    "    cleaned_docs = []\n",
    "    \n",
    "    # only looks at text between abstract and reference\n",
    "    documents = abbreviate(documents)\n",
    "    \n",
    "    # replaces new lines with a space\n",
    "    documents = strip_new_lines(documents)\n",
    "    \n",
    "    # replaces punctuation with a space\n",
    "    documents = strip_punctuation(documents)\n",
    "\n",
    "    # replaces numbers with a space\n",
    "    documents = strip_numbers(documents)\n",
    "   \n",
    "    # removes stop words\n",
    "    documents = strip_stopwords(documents)\n",
    "  \n",
    "    # removes words less than length 3\n",
    "    documents = strip_short_words(documents)\n",
    "    \n",
    "    cleaned_docs = documents\n",
    "\n",
    "    return cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling topic general topic model built tasa touchstone applied science associates corpus used study topic scores utterances chat data computed seven relevant topics selected based total document scores aggregated topic scores power predicting students learning using epistemic network analysis enables assessing data different angle results showed topic score based epistemic networks low gain students high gain students significantly different overall results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions keywords chat collaborative learning topic modeling epistemic network analysis introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synchronously asynchronously participate tasks accom\n"
     ]
    }
   ],
   "source": [
    "# 11a) reimport your raw data using the code in 2)\n",
    "documents = []\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file) as filename: \n",
    "        contents = filename.read()\n",
    "        documents.append(contents)\n",
    "\n",
    "        \n",
    "# 11b) clean your files using the function above\n",
    "clean = clean_list_of_documents(documents)\n",
    "\n",
    "# 11c) print the first 1000 characters of the first document\n",
    "print(clean[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Build your list of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of words (i.e., the vocabulary) is going to become the columns of your matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Describe why we need to figure out the vocabulary used in our corpus (refer back to Sherin's paper, and explain in your own words): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15947\n",
      "5649\n"
     ]
    }
   ],
   "source": [
    "# 13) create a function that takes in a list of documents\n",
    "# and returns a set of unique words. Make sure that you\n",
    "# sort the list alphabetically before returning it. \n",
    "\n",
    "def get_vocabulary(documents):\n",
    "    voc = []\n",
    "    for document in documents:\n",
    "        words = document.split()\n",
    "        for word in words:\n",
    "            if word not in voc:\n",
    "                voc.append(word)\n",
    "    voc.sort()\n",
    "    return voc\n",
    "\n",
    "# Then print the length of your vocabulary (it should be \n",
    "# around 5500 words)\n",
    "\n",
    "#cleaned\n",
    "print(len(get_vocabulary(documents)))\n",
    "#uncleaned\n",
    "print(len(get_vocabulary(clean)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) what was the size of Sherin's vocabulary? \n",
    "\n",
    "# full vocabulary was 1429 words, pruned list was 647 after removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - transform your documents into 100-words chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) create a function that takes in a list of documents\n",
    "# and returns a list of 100-words chunk \n",
    "# (with a 25 words overlap between them)\n",
    "# Optional: add two arguments, one for the number of words\n",
    "# in each chunk, and one for the overlap size\n",
    "# Advice: combining all the documents into one giant string\n",
    "# and splitting it into separate words will make your life easier!\n",
    "\n",
    "all_documents = ''.join(clean)\n",
    "\n",
    "def chunk(list_docs, chunk_size, overlap):\n",
    "    words = all_documents.split()\n",
    "    chunks = []\n",
    "    for word in range(0, len(words), overlap):\n",
    "        chunks.append(words[word:word + chunk_size])\n",
    "    return chunks\n",
    "    \n",
    "chunks = chunk(all_documents, 100, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "length is not equal to 100, length is 86",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-610ba1704638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m      \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"length is not equal to 100, length is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#     if len(chunk) != 100:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#         print(\"length is not equal to 100, length is \" + str(len(chunk)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: length is not equal to 100, length is 86"
     ]
    }
   ],
   "source": [
    "# 16) create a for loop to double check that each chunk has \n",
    "# a length of 100\n",
    "# Optional: use assert to do this check\n",
    "\n",
    "for chunk in chunks:\n",
    "     assert (len(chunk) == 100),\"length is not equal to 100, length is \" + str(len(chunk))\n",
    "#     if len(chunk) != 100:\n",
    "#         print(\"length is not equal to 100, length is \" + str(len(chunk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2229\n",
      "['study', 'investigates', 'possible', 'way', 'analyze', 'chat', 'data', 'collaborative', 'learning', 'environments', 'using', 'epistemic', 'network', 'analysis', 'topic', 'modeling', 'topic', 'general', 'topic', 'model', 'built', 'tasa', 'touchstone', 'applied', 'science', 'associates', 'corpus', 'used', 'study', 'topic', 'scores', 'utterances', 'chat', 'data', 'computed', 'seven', 'relevant', 'topics', 'selected', 'based', 'total', 'document', 'scores', 'aggregated', 'topic', 'scores', 'power', 'predicting', 'students', 'learning', 'using', 'epistemic', 'network', 'analysis', 'enables', 'assessing', 'data', 'different', 'angle', 'results', 'showed', 'topic', 'score', 'based', 'epistemic', 'networks', 'low', 'gain', 'students', 'high', 'gain', 'students', 'significantly', 'different', 'overall', 'results', 'suggest', 'two', 'analytical', 'approaches', 'provide', 'complementary', 'information', 'afford', 'new', 'insights', 'processes', 'related', 'successful', 'collaborative', 'interactions', 'keywords', 'chat', 'collaborative', 'learning', 'topic', 'modeling', 'epistemic', 'network', 'analysis']\n",
      "study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling topic general topic model built tasa touchstone applied science associates corpus used study topic scores utterances chat data computed seven relevant topics selected based total document scores aggregated topic scores power predicting students learning using epistemic network analysis enables assessing data different angle results showed topic score based epistemic networks low gain students high gain students significantly different overall results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions keywords chat collaborative learning topic modeling epistemic network analysis\n"
     ]
    }
   ],
   "source": [
    "# 17) print the first chunk, and compare it to the original text.\n",
    "# does that match what Sherin describes in his paper?\n",
    "print(len(chunks))\n",
    "print(chunks[0])\n",
    "print(all_documents[:811])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) how many chunks did Sherin have? What does a chunk become \n",
    "# in the next step of our topic modeling algorithm? \n",
    "\n",
    "# 794 chunks, which later become vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) what are some other preprocessing steps we could do \n",
    "# to improve the quality of the text data? Mention at least 2.\n",
    "\n",
    "# Using https://pypi.org/project/stemming/1.0/ to group words of the same root\n",
    "# Remove anything that is not strictly alphabetic \n",
    "# (there are still a couple of math symbols that were not included in punctuation stripping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20) in your own words, describe the next steps of the \n",
    "# data modeling algorithms (listed below):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Vector and Matrix operations\n",
    "\n",
    "- Sherin says \"At this stage of the analysis, each of the 794 segments\n",
    "has been mapped to a vector that is understood to represent the meaning\n",
    "of that segment. The next step is to identify common meanings among these\n",
    "segments. To do that, I look for natural clusterings of the 794 vectors.\"\n",
    "\n",
    "- In other words, map segments to vectors, where the vector represents the \n",
    "meaning of the segment\n",
    "\n",
    "## Step 6 - Weight word frequency\n",
    "\n",
    "- Sherin says \"In most vector space analyses, the raw counts\n",
    "are modified by a weighting function. In the analyses reported in this article,\n",
    "each count is replaced with (1 + log[count]). This has the effect of dampening\n",
    "the impact of very frequent words. (Raw counts of zero are just left as zero.)\"\n",
    "\n",
    "- In other words, scaling raw counts by way of a weighting function for word \n",
    "frequencies to account for the most frequent words \n",
    "\n",
    "## Step 7 - Matrix normalization\n",
    "\n",
    "- Sherin says \"Finally, all of the vectors used in the present work are normalized. This means\n",
    "that every entry in the vector is divided by a constant (the length of the vector)\n",
    "so that the resulting vector has a length of 1.\"\n",
    "\n",
    "- In other words, making the length of the vectors the same so comparisons are easier across \n",
    "all vectors\n",
    "\n",
    "## Step 8 - Deviation Vectors\n",
    "\n",
    "- Sherin says \"For that purpose, I compute what I call deviation vectors. To compute the deviation\n",
    "vectors for two vectors V1 and V2, I first find their average and then break\n",
    "each vector into two components, one that lies along the average and another that\n",
    "is perpendicular to the average (refer to Figure 4). The perpendicular components,\n",
    "V1‚Äô and V2‚Äô, are the deviation vectors. If we use these deviation vectors in place\n",
    "of the original vectors, the result is that V1 and V2 have each been replaced by the\n",
    "component that defines its unique piece‚Äîa piece that characterizes how it differs\n",
    "from the average.\"\n",
    "\n",
    "- In other words, this step is sort of like calculating standard deviation. We are only concerned\n",
    "with how far each vector is from the average as opposed to the actual vectors being compared\n",
    "\n",
    "## Step 9 - Clustering\n",
    "\n",
    "- Sherin says \"To cluster the transcript vectors, I employed the very general technique called\n",
    "hierarchical agglomerative clustering. In hierarchical agglomerative clustering,\n",
    "the analysis begins with each of the items in its own cluster. Thus, we begin with\n",
    "a number of clusters equal to the total number of items. Then two of those clusters\n",
    "are combined into a single cluster containing two items, thus reducing the total\n",
    "number of clusters by one. The process then iterates: Two clusters are combined\n",
    "and the total number of clusters is decreased by one. This repeats until all of the\n",
    "items are combined into a single cluster. The result is a list of candidate clusterings\n",
    "of the data, with each candidate corresponding to one of the intermediate steps in\n",
    "this process...The results I describe were obtained using a technique\n",
    "called centroid clustering. At each step in the iteration, I first find the centroid\n",
    "of each cluster (the average of all of the vectors currently in the cluster). Then\n",
    "I find the pair of centroids that are closest to each other and merge the associated\n",
    "clusters. An explanation of centroid clustering, including its application to\n",
    "LSA-produced vectors, can be found in Manning, Raghavan, and SchuÃàtze (2008).\"\n",
    "\n",
    "- In other words, there are two methods for clustering \"hierarchical agglomerative clustering\"\n",
    "and \"centroid clustering\". Centroid clustering was used in Sherin's example because it was deterministic,\n",
    "\"hierarchical agglomerative clustering\" involves some heuristic for combining clusters so that it is done\n",
    "systematically.\n",
    "\n",
    "## Step 10 - Visualizing the results\n",
    "\n",
    "- Create visuals to help others understand the data and findings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step - Putting it all together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
